# Real-Time Voice Sentiment Visualization

A full-stack web application that performs real-time audio transcription and visualizes the speaker's emotional sentiment through a beautiful, dynamic Perlin noise field.

Try it out here: https://voice-agent-ruddy.vercel.app/

## ğŸ¨ Features

- **Real-time Audio Transcription**: Uses Deepgram API for high-accuracy speech-to-text
- **AI-Powered Sentiment Analysis**: Leverages OpenAI GPT-4 or Anthropic Claude to extract sentiment, emotion, and keywords
- **Dynamic Perlin Noise Visualization**: Gorgeous generative art that responds to emotional data
  - Color shifts based on sentiment type (joy, calm, anxiety, anger, etc.)
  - Particle energy reflects emotional intensity
  - Flow field dynamics respond to energy levels
- **Smooth Animations**: Keywords fade in gracefully, transcripts auto-scroll
- **Modern UI**: Clean, semi-transparent overlays with glassmorphism effects

## ğŸ—ï¸ Architecture

This is a three-part system:

1. **Frontend (React)**: Captures audio, manages WebSocket connections, displays UI and visualization
2. **Backend (FastAPI)**: Proxy server that securely calls AI APIs for sentiment analysis
3. **External APIs**: 
   - Deepgram for real-time transcription
   - OpenAI/Claude for sentiment and keyword extraction

## ğŸ“‹ Prerequisites

- Node.js (v16 or higher)
- Python 3.8+
- Deepgram API Key ($200 credits available)
- OpenAI API Key or Anthropic API Key

## ğŸš€ Setup Instructions

### 1. Backend Setup

```bash
cd backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
# Create a .env file in the backend directory:
echo "OPENAI_API_KEY=your_key_here" > .env
# OR
echo "ANTHROPIC_API_KEY=your_key_here" > .env

# Run the server
python main.py
```

The backend will be available at `http://localhost:8000`

### 2. Frontend Setup

```bash
cd voice-agent

# Install dependencies
npm install

# Set up environment variables
# Create a .env file in the voice-agent directory:
echo "REACT_APP_DEEPGRAM_API_KEY=your_key_here" > .env
echo "REACT_APP_API_URL=http://localhost:8000" >> .env

# Start the development server
npm start
```

The app will open at `http://localhost:3000`

## ğŸ® Usage

1. Click **"Start Recording"** to begin capturing audio
2. Speak naturally - your words will appear in the transcript panel
3. Watch the visualization respond to your emotional tone
4. Keywords will fade in smoothly on the right panel
5. Click **"Stop Recording"** when finished

## ğŸ¨ Visualization Details

The Perlin noise visualization maps sentiment data to visual parameters:

- **Color (Hue)**:
  - Joyful/Happy: Yellow-orange (45Â°)
  - Calm/Peaceful: Blue (200Â°)
  - Anxious/Nervous: Purple (280Â°)
  - Angry: Red (0Â°)
  - Sad: Deep blue (220Â°)
  - Surprised: Cyan (160Â°)
  - Loving: Pink (330Â°)

- **Saturation**: Increases with emotion intensity
- **Brightness**: Higher for more intense emotions
- **Particle Speed**: Scales with energy level (calm â†’ energetic)
- **Flow Field Complexity**: More turbulent for higher energy

## ğŸ“ Project Structure

```
voice-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ PerlinVisualization.js    # P5.js Perlin noise field
â”‚   â”‚   â”œâ”€â”€ TranscriptDisplay.js       # Live transcript panel
â”‚   â”‚   â”œâ”€â”€ KeywordsDisplay.js         # Animated keywords
â”‚   â”‚   â””â”€â”€ Controls.js                # Start/Stop buttons
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ useDeepgram.js             # Deepgram WebSocket management
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ sentimentService.js        # Backend API calls
â”‚   â”œâ”€â”€ App.js                         # Main application
â”‚   â””â”€â”€ index.js                       # Entry point
â”œâ”€â”€ public/
â””â”€â”€ package.json

backend/
â”œâ”€â”€ main.py                            # FastAPI server
â”œâ”€â”€ requirements.txt                   # Python dependencies
â””â”€â”€ .env                               # API keys (create this)
```

## ğŸ”§ Configuration

### Backend API Model Selection

By default, the app uses OpenAI GPT-4. To switch to Claude:

1. Update `sentimentService.js` to call `/process_text_claude`
2. Set `ANTHROPIC_API_KEY` in backend `.env`

### Visualization Tuning

Adjust parameters in `PerlinVisualization.js`:
- `particlesRef.current`: Number of particles (default: 500)
- Flow field resolution: `cols` and `rows` (default: 20px grid)
- Color transition speed: `0.05` in the lerp calculation
- Trail effect: Alpha value in background rect

## ğŸ› Troubleshooting

**Microphone not working:**
- Ensure HTTPS or localhost (mic requires secure context)
- Check browser permissions for microphone access

**Backend connection failed:**
- Verify backend is running on port 8000
- Check CORS settings in `main.py`
- Ensure `.env` variables are set correctly

**Deepgram errors:**
- Verify API key is valid and has credits
- Check internet connection for WebSocket

**No visualization:**
- Open browser console for errors
- Ensure `react-p5` and `p5` are installed

## ğŸ¯ Assessment Criteria

This project demonstrates:

âœ… **Full-Stack Orchestration**: Frontend, backend, and two external APIs working in harmony  
âœ… **Data-Driven Visualization**: Sentiment mapped to color, motion, and intensity  
âœ… **Frontend Polish**: Smooth transitions, fade-in animations, glassmorphism  
âœ… **Async Management**: Debouncing, error handling, loading states  
âœ… **Real-time Performance**: WebSocket streaming, responsive updates  

## ğŸ“š Technologies Used

- **Frontend**: React, react-p5, p5.js, Deepgram SDK, Axios
- **Backend**: FastAPI, OpenAI/Anthropic SDK, Uvicorn
- **APIs**: Deepgram (transcription), OpenAI GPT-4 / Claude (sentiment)
- **Styling**: CSS3 with glassmorphism, animations, gradients

## ğŸŒŸ Future Enhancements

- [ ] Multi-language support
- [ ] Export transcript and sentiment timeline
- [ ] 3D visualization with Three.js
- [ ] Voice emotion training mode
- [ ] Real-time sentiment graph overlay
- [ ] Multiple speaker detection

## ğŸ“„ License

This project is built as a technical demonstration for Memory Machines.

---

**Built with â¤ï¸ for Memory Machines - Going Beyond LLMs**
